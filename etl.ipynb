{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Data wrangling of digital assets\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "The project leveraging financial and social data related to the crypto currency industry. The idea is to prepare the data to e.g. explore the relationship between price trends of cryptocurrency assets (e.g. Bitcoin) and its sentiment from social media platforms. \n",
    "Hereby, structured financial market data (e.g. Bitcoin price over a year) as well as unstructured social media data (e.g. Bitcoin tweets on twitter) will be extracted from the source platforms (e.g. cryptocurrency exchange or twitter). Then the data will be transformed (e.g. remove duplicates, denormailze tables). Finally, the prepared data will be loaded to the target system (e.g. filesystem)\n",
    "\n",
    "Hereby, following steps are executed as part of the ETL process:\n",
    "* 1. Extract and transform data from cryptocurrecny exchange\n",
    "    * 1.1 Extract tradingdata\n",
    "    * 1.2 Transform trading data to marketdata\n",
    "* 2. Extract and transform social data from Twitter\n",
    "    * 2.1 Extract tweets from twitter and persist original tweets on filesystem\n",
    "    * 2.2 Extract orginal tweets from filesystem\n",
    "    * 2.3 Transform original tweets\n",
    "* 3. Run quality checks on tweets and marketdata\n",
    "* 4. Persist tweets and marketdata on filesystem\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Libraries\n",
    "At this step necessary libraries being imported. Note, to be able to import Twitter API package the TwitterAPI wrapper need to be installed first running \"pip install TwitterApi\" command in the bash console.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "## Import dependencies\n",
    "\n",
    "%load_ext sql\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "import datetime\n",
    "import time\n",
    "import json\n",
    "import csv\n",
    "\n",
    "from urllib.parse import urljoin\n",
    "from functools import reduce\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import SparkSession, DataFrame, Row\n",
    "from pyspark.sql.functions import year, month, dayofweek, hour, weekofyear, date_format, \\\n",
    "                                  udf, col, lit, struct, isnan, count, when\n",
    "\n",
    "from TwitterAPI import TwitterAPI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Spark environment\n",
    "As the application is running locally on jupyter notebook the Spark Session has to  be instantiated manually. In case the application is executed in an environment leveraging a Spark kernel (e.g. Amazon EMR) leveraging this step is not needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "## Make spark environment available.\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:2.7.0\") \\\n",
    "    .appName(\"ETL_CryptocurrencyInsight\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### 1. Extract and transform data from cryptocurrecny exchange\n",
    "In this step the data extraction (1.1) and transformation (1.2) of the trading data is executed. As part of the data extraction the trading data for Bitcoin and Ethereum are fetched from the cryptocurrency exchange. Please note that the trading details refer to the \"daily close\" containing aggregated data of one day of trading. \n",
    "#### 1.1 Extract tradingdata \n",
    "To extract the tradingdata the public API from the cryptocurrency exchange \"Binance\" was taken. The cryptocurrency exchange binance is one of the most popular exchanges with the highest volume in terms of trading volumne. Furthermore, it provides a public api allowing to execute GET request to receive trading details. Therefore, the binance API is selected as datasource. The Get Request fetches data about the daily close (closing price, highest price etc.) of Bitcoin (BTCUSDT) and Ethereum (ETHUSDT).\n",
    "\n",
    "#### 1.2 Transform tradingdata to marketdata\n",
    "After the tradingdata have been extracted succesfully the tradingdata are transformed to marketdata. As part of the transformation the data are cleaned by removing nan and null values. Furthermore, duplicates will be removed. Finally the original tradingdata are splitted into three dataframes denormalizing time (e.g. datetime), trade (e.g. volume) and symbol (e.g. BTCUSD) related data. Based on the closing time a unqiue id is being created. The id describes the relation between the spark dataframes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success: Call to crypto currency exchange executed\n",
      "Success: Trading data were extracted\n",
      "Success: Call to crypto currency exchange executed\n",
      "Success: Trading data were extracted\n",
      "Success: Call to crypto currency exchange executed\n",
      "Success: Trading data were extracted\n",
      "Success: Marketdata were transformed\n"
     ]
    }
   ],
   "source": [
    "def extract_tradingdata(symbol):\n",
    "    \"\"\"\n",
    "    Description: Collect URL and all parameters. The interval is set to 1d fetching the daily close.\n",
    "                 No specific headers needed as it is a public API.\n",
    "                 Note: To avoid IP ban the number of API calls muste be < 100 calls/minute.\n",
    "    Parameters:\n",
    "        symbol: Describes the traidingpair (cryptocurrency asset) for which trading data being requested.\n",
    "    Returns:\n",
    "        response: Contains the response from the API call to the cryptocurrency exchange.\n",
    "    \"\"\"\n",
    "    \n",
    "    baseurl = \"https://api.binance.com\"\n",
    "    path = \"/api/v3/klines\"\n",
    "    headers = None\n",
    "    params = {\n",
    "        'symbol': symbol,\n",
    "        'interval': '1d'\n",
    "    }\n",
    "    url = urljoin(baseurl, path)\n",
    "    \n",
    "    # Requesting data from cryptocurrency exchange for specific symbol\n",
    "    response = requests.get(url, headers=headers, params=params)\n",
    "    \n",
    "    print(\"Success: Call to crypto currency exchange executed\")\n",
    "    return response\n",
    "    \n",
    "def get_tradingdata(symbol):\n",
    "    \"\"\"\n",
    "    Description: The function is calling the API of an cryptocurrency exchange getting the trading details for\n",
    "                 for a specific cryptocurrency asset. Afterwards it transforms the trading data.\n",
    "    Parameters:\n",
    "        symbol: Describes the traidingpair (cryptocurrency asset) for which trading data being requested.\n",
    "    Returns:\n",
    "        df_marketdata: Spark dataframe containing the trading data which are matched with the corresponding symbol.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Execute the trading data extraction from datasource\n",
    "    response = extract_tradingdata(symbol)\n",
    "\n",
    "    # If the response is successful and data are being fetched prepare dataframe containing marketdata\n",
    "    if response.status_code == 200:\n",
    "\n",
    "        # Extract trading details and make them available to the application\n",
    "        # Define columns for dataframe based on API description from cryptocurrency exchange. \n",
    "        # Source: https://github.com/binance-exchange/binance-official-api-docs/blob/master/rest-api.md#general-api-information\n",
    "        content_tradingdata = response.json()   \n",
    "        columns_tradingdata = ['open_time', \n",
    "                              'open', \n",
    "                              'high', \n",
    "                              'low', \n",
    "                              'close', \n",
    "                              'volume', \n",
    "                              'close_time', \n",
    "                              'quote_asset_volume', \n",
    "                              'number_of_trades', \n",
    "                              'taker_buy_base_asset_volume', \n",
    "                              'taker_buy_quote_asset_volume', \n",
    "                              'unclassfied']\n",
    "        df_tradingdata = spark.createDataFrame(content_tradingdata, columns_tradingdata)\n",
    "\n",
    "        # Overwritting trading dataframe by adjusting datatypes and adding symbol column\n",
    "        # ensuring the trading details are assigned to the correct cryptocurrency\n",
    "        df_tradingdata = df_tradingdata.selectExpr(\"cast(open_time as long) open_time\", \n",
    "                                                  \"cast(open as decimal(20,10)) open\", \n",
    "                                                  \"cast(high as decimal(20,10)) high\", \n",
    "                                                  \"cast(low as decimal(20,10)) low\", \n",
    "                                                  \"cast(close as decimal(20,10)) close\",\n",
    "                                                  \"cast(volume as decimal(20,10)) volume\",\n",
    "                                                  \"cast(close_time as long) close_time\",\n",
    "                                                  \"cast(quote_asset_volume as decimal(20,10)) quote_asset_volume\",\n",
    "                                                  \"cast(number_of_trades as integer) number_of_trades\",\n",
    "                                                  \"cast(taker_buy_base_asset_volume as decimal(20,10)) taker_buy_base_asset_volume\",\n",
    "                                                  \"cast(taker_buy_quote_asset_volume as decimal(20,10)) taker_buy_quote_asset_volume\") \\\n",
    "                                                  .withColumn(\"symbol\", lit(symbol))\n",
    "        \n",
    "        print(\"Success: Trading data were extracted\")\n",
    "        return df_tradingdata\n",
    "\n",
    "    else:\n",
    "        print(\"Crypto Currency API not available. Please try later\")  \n",
    "\n",
    "def get_marketdata(trading_pair_symbols):       \n",
    "    \"\"\"\n",
    "    Description: Creates a dataframe containing trading data for all requested cryptocurrency assets (marketdata).\n",
    "    Parameters: \n",
    "        trading_pair_symbols: List containing cryptocurrency assets symbols.\n",
    "    Returns: \n",
    "        None\n",
    "    \"\"\"\n",
    "    \n",
    "    # Merges dataframes containing trading details for each cryptocurrency into one dataframe containing all data (marketdata)\n",
    "    series_tradingdata = []\n",
    "    for trading_pair_symbol in trading_pair_symbols:\n",
    "        series_tradingdata.append(get_tradingdata(trading_pair_symbol))\n",
    "    \n",
    "    # Execute DAG and combine collection of trading data dataframes \n",
    "    return reduce(DataFrame.unionAll, series_tradingdata)\n",
    "\n",
    "def transform_marketdata(df_marketdata):       \n",
    "    \"\"\"\n",
    "    Description: Process marketdata and split it into three tables following star schema approach.\n",
    "                 The fact table (df_marketdata_trade) created contains trading metrics. The time table \n",
    "                 (df_marketdata_time) is descriptive and contains related trading times. The symbol \n",
    "                 table (df_marketdata_symbol) contains symbols (trading pairs). The relation between the\n",
    "                 dataframes is described by the closing time (id).             \n",
    "    Parameters: \n",
    "        df_marketdata: Contains all marketdata as result from data extraction\n",
    "    Returns: \n",
    "        None\n",
    "    \"\"\"\n",
    "    # @udf: Extract datetime based on the timestamp (in ms)\n",
    "    get_datetime_udf = udf(lambda ts: datetime.datetime.fromtimestamp(ts / 1000.0).strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "    \n",
    "    # Convert timestamp to datetime and rename closing time to id\n",
    "    # Note: The closing time acts as id as it is unique and allows to combine different marketdata dataframes \n",
    "    df_marketdata = df_marketdata.withColumn(\"datetime\", get_datetime_udf(df_marketdata.close_time)) \\\n",
    "                                 .withColumnRenamed(\"close_time\",\"id\")\n",
    "    \n",
    "    # Ensure data quality dropping null values and duplicates in id. \n",
    "    df_marketdata.na.drop(subset=[\"id\"]).dropDuplicates([\"id\"])\n",
    "    \n",
    "    # Create time table dataframe for marketdata. The id is expressed by the closing time. \n",
    "    df_marketdata_time = df_marketdata.select([\"id\",\n",
    "                                               month(\"datetime\").alias(\"month\"),\n",
    "                                               year(\"datetime\").alias(\"year\"),\n",
    "                                               dayofweek(\"datetime\").alias(\"weekday\"),\n",
    "                                               weekofyear(\"datetime\").alias(\"weekofyear\"),\n",
    "                                               \"datetime\"])\n",
    "    \n",
    "    # Create dataframe trade table containing all revelant details.\n",
    "    df_marketdata_trade = df_marketdata.select(\"id\",\n",
    "                                               \"open\",\n",
    "                                               \"close\",\n",
    "                                               \"high\",\n",
    "                                               \"low\",\n",
    "                                               \"quote_asset_volume\",\n",
    "                                               \"number_of_trades\",\n",
    "                                               \"taker_buy_base_asset_volume\",\n",
    "                                               \"taker_buy_quote_asset_volume\")\n",
    "    \n",
    "    # Create dataframe trade table containing all revelant details.\n",
    "    df_marketdata_symbol = df_marketdata.select(\"id\",\n",
    "                                                \"symbol\")\n",
    "    \n",
    "    print(\"Success: Marketdata were transformed\")\n",
    "    return df_marketdata_time, df_marketdata_trade, df_marketdata_symbol\n",
    "\n",
    "## Receive all marketdata for cryptocurrency assets\n",
    "trading_pair_symbols = [\"BTCUSDT\", \"ETHUSDT\", \"XRPUSDT\"]\n",
    "df_marketdata = get_marketdata(trading_pair_symbols)\n",
    "\n",
    "## Split the marketdata into different tables following star schema\n",
    "df_marketdata_time, df_marketdata_trade, df_marketdata_symbol = transform_marketdata(df_marketdata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### 2. Extract and transform social data from Twitter\n",
    "In the following steps the twitter tweets about bitcoin for a specific time period (23.10.2019 to 26.10.2019) are extracted, transformed and loaded to a filesytem.\n",
    "As the search and result limit of the Twitter API is restricted for search results and requests the data were stored on the file system in json format. Note that this step was necessary to decouple the internal ETL pipeline from the Twitter API. \n",
    "\n",
    "### Excursus Twitter API/App: \n",
    "To be able to extract tweets from Twitter a twitter app need to be configured. During the configuration process the necessary credentials are generated. While setting up the Twitter app, the premium API from Twitter is selected. The premium api allows to executed twitter searches for specific period (e.g 23.10.22019 to 26.10.2019). While the standard api allows only to request data from last the 7 days. More information on different Twitter APIs can be found here: https://developer.twitter.com/en/premium-apis.html\n",
    "\n",
    "#### 2.1 Extract tweets from twitter and persist original tweets on filesystem\n",
    "To extract bitcoin tweets, the Twitter Search API is applied. Note that the Twitter Search API is non-public. Therefore, credentials are provided when executing the get request. Hereby, existing twitter tweets between the 23.10.22019 to 26.10.2019 are extracted. The period was choosen as the bitcoin price was extremely volatile in this period.   \n",
    "After the tweets were extracted the complex json object was flattened and persisted on a storage system. As part of the data cleaning only meaningful tweet attributes are kept. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def setup_twitterAPI():       \n",
    "    \"\"\"\n",
    "    Description:  Calls the twitter api wrapper and provides credentials to it.         \n",
    "    Parameters: \n",
    "        None\n",
    "    Returns: \n",
    "        twitterAPI: The prepared API wrapper allowing to send request to twitter api\n",
    "    \"\"\"\n",
    "    # Provide all credentials to access the twitter API\n",
    "    consumer_key = \"\"\n",
    "    consumer_secret = \"\"\n",
    "    access_token_key = \"\"\n",
    "    access_token_secret = \"\"\n",
    "\n",
    "    # Get the API from the Twitter API wrapper and provide credentials\n",
    "    twitterAPI = TwitterAPI(consumer_key, \n",
    "                            consumer_secret, \n",
    "                            access_token_key, \n",
    "                            access_token_secret)\n",
    "    \n",
    "    print(\"Success: Twitter API was configured\")\n",
    "    return twitterAPI\n",
    "    \n",
    "def get_parameters():       \n",
    "    \"\"\"\n",
    "    Description: Define all needed parameters to execute a twitter tweet search.\n",
    "    Parameters: \n",
    "        None\n",
    "    Returns: \n",
    "        parameters: Dictionary containing relevant search parameters\n",
    "    \"\"\"\n",
    "    \n",
    "    # The string in the search term contains a special parameter lang: which requesting only english tweets.\n",
    "    search_term = \"bitcoin lang:en\"\n",
    "    \n",
    "    # Defines the twitter product being used. The \"30 day\" product allows to access the last 30 days.\n",
    "    product = \"30day\"\n",
    "    \n",
    "    # Name of the twitter application defined.\n",
    "    label = \"prototype\"\n",
    "    \n",
    "    #  The twitter search begins at the startDate and stops at the endDate. However, the search may stop before\n",
    "    #  in case the number of allowed request is reached (30/minute or 250/month).\n",
    "    startDate = \"201910250000\" \n",
    "    endDate = \"201910260000\"\n",
    "    \n",
    "    # The nextpage_token is necessary to enable pagination\n",
    "    nextpage_token = None\n",
    "\n",
    "    parameters = {\"label\": label,\n",
    "                  \"product\": product,\n",
    "                  \"query\": search_term,\n",
    "                  \"fromDate\": startDate, \n",
    "                  \"toDate\": endDate,\n",
    "                  \"next\": nextpage_token}\n",
    "    \n",
    "    print(\"Success: Parameters for Twitter Api were defined\")\n",
    "    return parameters\n",
    "\n",
    "def get_twitterdata():       \n",
    "    \"\"\"\n",
    "    Description: Execute get request and save received tweets a dictionary containing only releveant data from tweet. \n",
    "                 Note: The results and requests of the twitter api are strictly limited allowing 30 requests/minute and \n",
    "                 250 requests/month. In addition only 100 are shown per request (page). Therefore, the request need to be splitted \n",
    "                 (e.g. based on date) and executed several times applying next token.\n",
    "                 Source:          \n",
    "    Parameters: \n",
    "        None\n",
    "    Returns: \n",
    "        cryptocurrency_tweets_pd: Dataframe containing all relevant cryptocurrency tweets for the desired time period\n",
    "    \"\"\"\n",
    "    \n",
    "    # Make Twitter Api available and define parameters for twitter search\n",
    "    twitterAPI = setup_twitterAPI()\n",
    "    parameters = get_parameters()\n",
    "    \n",
    "    # As the request is a nested json object containing data which are not needed\n",
    "    # the relevant details are extracted and persisted in a dictionary \n",
    "    cryptocurrency_tweets_dict = {\"id\": [], \n",
    "                                  \"date\": [], \n",
    "                                  \"text\": [],\n",
    "                                  \"user\": [], \n",
    "                                  \"favorite_count\": [], \n",
    "                                  \"retweet_count\": [], \n",
    "                                  \"followers_count\": []}\n",
    "\n",
    "    # Execute actual twitter search based on search term considering pagination and english language\n",
    "    # Filter out unnesseary tweet data and create dictionary containing relevant data\n",
    "    while True:\n",
    "        request = twitterAPI.request(\"tweets/search/%s/:%s\" % (parameters[\"product\"], parameters[\"label\"]),\n",
    "                                                              {\"query\": parameters[\"query\"],\n",
    "                                                               \"fromDate\": parameters[\"fromDate\"],\n",
    "                                                               \"toDate\": parameters[\"toDate\"],\n",
    "                                                               \"next\": parameters[\"next\"]})\n",
    "        twitter_tweets = request.json()\n",
    "        if (request.status_code != 200):\n",
    "            print(\"Error: Twitter API Call was not successful\")\n",
    "            request_status = False\n",
    "            break\n",
    "        for twitter_tweet in twitter_tweets[\"results\"]:\n",
    "            cryptocurrency_tweets_dict[\"id\"].append(twitter_tweet[\"id\"])\n",
    "            cryptocurrency_tweets_dict[\"date\"].append(twitter_tweet[\"created_at\"])\n",
    "            cryptocurrency_tweets_dict[\"text\"].append(twitter_tweet[\"text\"])\n",
    "            cryptocurrency_tweets_dict[\"user\"].append(twitter_tweet[\"user\"][\"screen_name\"])\n",
    "            cryptocurrency_tweets_dict[\"favorite_count\"].append(twitter_tweet[\"favorite_count\"])\n",
    "            cryptocurrency_tweets_dict[\"retweet_count\"].append(twitter_tweet[\"retweet_count\"])\n",
    "            cryptocurrency_tweets_dict[\"followers_count\"].append(twitter_tweet[\"user\"][\"followers_count\"])\n",
    "        if \"next\" not in twitter_tweets:\n",
    "            print(\"Success: Twitter API Call executed\")\n",
    "            request_status = True\n",
    "            break\n",
    "        parameters[\"next\"] = twitter_tweets[\"next\"]\n",
    "    \n",
    "    # To ease up post-processing use pandas to create dataframe containing all relevant cryptocurrency tweets\n",
    "    cryptocurrency_tweets_pd = pd.DataFrame(cryptocurrency_tweets_dict)\n",
    "    return cryptocurrency_tweets_pd, request_status\n",
    "\n",
    "def persist_twitterdata(df_twitterdata, request_status):\n",
    "    \"\"\"\n",
    "    Description: Persist the tweets in json format on the filesystem. This is necessary to decouple the from twitter api\n",
    "                 due to result and call limitations. In case the Twitter Api limit is reached, no data will be persisted. \n",
    "                 In that case refer to existing data in datasource/twitter_tweets folder.\n",
    "    Parameters: \n",
    "        df_twitterdata: Panda dataframe containing tweets for a specific period in the past\n",
    "        request_status: In case the request was executed succesfully the status is set to True. Otherwise False. \n",
    "    Returns: \n",
    "        None\n",
    "    \"\"\"    \n",
    "    \n",
    "    if request_status == True:\n",
    "        cryptocurrency_tweets_df = spark.createDataFrame(cryptocurrency_tweets_pd)\n",
    "        cryptocurrency_tweets_df.write.json(path=\"data/twitter_tweets\", \n",
    "                                            mode=\"append\")\n",
    "        print(\"Success: Tweets were written into filesystem\")\n",
    "    else:\n",
    "        print(\"Error: Tweets were not written into filesystem\")\n",
    "\n",
    "## Access twitter (30 day premium) api and receive tweets\n",
    "df_twitterdata, request_status = get_twitterdata()\n",
    "\n",
    "## Store twitter data on a local filesystem\n",
    "persist_twitterdata(df_twitterdata, request_status)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### 2. Extract and transform social data from Twitter\n",
    "In the following to steps the original tweets which have been stored on a filesystem are extracted and transformed. \n",
    "\n",
    "#### 2.2 Extract original tweets from filesystem\n",
    "Here the tweets from the internal filesystem are taken and read into an spark dataframe. \n",
    "        \n",
    "#### 2.3 Transform tweets\n",
    "As soon as step 2.2 was executed the tweets can be cleaned by removing duplicates, nan as and null values (based on id). Furthermore, the collection of tweets is denormalized and pushed into three different dateframes. Hereby, the dataframes can be seperated into user, content and time based dataframes. The relation between the dataframes is described by there id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success: Twitter tweets were extracted\n",
      "Success: Tweet dataframes were transformed\n"
     ]
    }
   ],
   "source": [
    "def extract_twitterdata():       \n",
    "    \"\"\"\n",
    "    Description: Load twitter data from external storage into a spark dataframe making it available for\n",
    "                 denormalization\n",
    "    Parameters: \n",
    "        None\n",
    "    Returns: \n",
    "        cryptocurrency_tweets_df: Spark dataframe containing twitter tweets\n",
    "    \"\"\"\n",
    " \n",
    "    # Ensure proper data structure by providing custom schema. Make data structure more robust\n",
    "    # by ensuring that a value for id is provided\n",
    "    fields = [StructField(\"id\", LongType(), False),\n",
    "              StructField(\"date\", StringType(), True),\n",
    "              StructField(\"text\", StringType(), True),\n",
    "              StructField(\"user\", StringType(), True), \n",
    "              StructField(\"favorite_count\", LongType(), True),\n",
    "              StructField(\"retweet_count\", LongType(), True),\n",
    "              StructField(\"followers_count\", LongType(), True)]\n",
    "                 \n",
    "    final_schema = StructType(fields)  \n",
    "    \n",
    "    # Read twitter data provided as json following the schema definition\n",
    "    cryptocurrency_tweets_df = spark.read.json(path=\"datasource/twitter_tweets\", schema=final_schema)\n",
    "    print(\"Success: Twitter tweets were extracted\")\n",
    "    return cryptocurrency_tweets_df\n",
    "\n",
    "## Process and denormalize tweets \n",
    "def transform_twitterdata(tweets_df):       \n",
    "    \"\"\"\n",
    "    Description:              \n",
    "    Parameters: \n",
    "        transform_twitterdata: Contains all cryptocurrency tweets from filesystem\n",
    "    Returns: \n",
    "        None\n",
    "    \"\"\"  \n",
    "    \n",
    "    # @udf: Convert ctime format to datetime and remove UTC timezone value as utc time is applicable for the entire project\n",
    "    get_time_value_sequence_datetime = udf(lambda dateTimeUtc: datetime.datetime.strptime(dateTimeUtc, \"%a %b %d %H:%M:%S %z %Y\") \\\n",
    "                                                                                .strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "    \n",
    "    # Ensure data quality dropping null values and duplicates in id. \n",
    "    tweets_df.na.drop(subset=[\"id\"]).dropDuplicates([\"id\"])\n",
    "    \n",
    "    # Convert ctime format to datetime allowing to extract date segments within dataframe\n",
    "    tweets_df = tweets_df.withColumn(\"datetime\", get_time_value_sequence_datetime(tweets_df.date)) \n",
    "    \n",
    "    # Contains time related information for each tweet splitted into time segments.\n",
    "    tweets_time_df = tweets_df.select([\"id\",\n",
    "                                       month(\"datetime\").alias(\"month\"),\n",
    "                                       year(\"datetime\").alias(\"year\"),\n",
    "                                       dayofweek(\"datetime\").alias(\"weekday\"),\n",
    "                                       weekofyear(\"datetime\").alias(\"weekofyear\"),\n",
    "                                       \"datetime\"])\n",
    "    \n",
    "    # Contains username and how many followers belong to a user\n",
    "    tweets_user_df = tweets_df.select(\"id\",\n",
    "                                      \"user\",\n",
    "                                      \"followers_count\")\n",
    "    \n",
    "    # Contains content related to an usertweet like the tweet message and how many times it was liked and retweeted.\n",
    "    tweets_content_df = tweets_df.select(\"id\",\n",
    "                                         \"text\",\n",
    "                                         \"favorite_count\",\n",
    "                                         \"retweet_count\")\n",
    "    \n",
    "    # Return dataframes as tuple for post-processing\n",
    "    print(\"Success: Tweet dataframes were transformed\")\n",
    "    return tweets_time_df, tweets_user_df, tweets_content_df\n",
    "\n",
    "## Extract tweets from filesystem\n",
    "tweets_df = extract_twitterdata()  \n",
    "## Process cryptocurrency tweets and denormalize tweets into time, user and content tweets\n",
    "tweets_time_df, tweets_user_df, tweets_content_df = transform_twitterdata(tweets_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### 3. Run quality checks on tweets and marketdata\n",
    "Before the data are persisted dedicated quality checks are executed on the market- and  twitterdata. Hereby, the quality checks are running on all six dataframes which have been created as outcome of the transformation process.\n",
    "In case no records are available the data quality check won't pass. Furthermore, the quality check highlights dataframes with low data quality due to nan or null values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Quality High. No action required\n",
      "Data Quality High. No action required\n",
      "Data Quality High. No action required\n",
      "Data Quality High. No action required\n",
      "Data Quality High. No action required\n",
      "Data Quality High. No action required\n"
     ]
    }
   ],
   "source": [
    "def data_quality_report(data_quality_df):\n",
    "    \"\"\"\n",
    "    Description: Gives a statement about the data quality. A high number of Nan or Null values results in a low quality\n",
    "                 of the respective dataframe\n",
    "    Parameters: \n",
    "        data_quality_df: Dataframe containing results of the quality check.\n",
    "    Returns: \n",
    "        None\n",
    "    \"\"\"    \n",
    "    for column in data_quality_df.columns:\n",
    "        if data_quality_df.filter(data_quality_df[column] != 0).count() > 0:\n",
    "            print(\"Data Quality Low. Check report\")\n",
    "            data_quality_df.show()\n",
    "            break\n",
    "        else:\n",
    "            None\n",
    "\n",
    "    print(\"Data Quality High. No action required\")\n",
    "\n",
    "def data_quality_check(data_df):\n",
    "    \"\"\"\n",
    "    Description: Validate the quality of the dataframes and report quality issues in case it contain null or nan entries\n",
    "                 Note: If the record count is 0 do not save the dataframe on the file system\n",
    "                       In case the the columns (except \"id\") does contain nan/null values store the dataframe on filesystem but\n",
    "                       report low data quality. In all other cases report high data quality and proceed.\n",
    "    Parameters: \n",
    "        data_df: Dataframe which will be checked on its quality\n",
    "    Returns: \n",
    "        Boolean: In case quality check was succesful True will be returned.\n",
    "    \"\"\"    \n",
    "\n",
    "    # Count available records ensuring records are available\n",
    "    count_records = data_df.count()\n",
    "    \n",
    "    # Create data quality dataframe aggregating available nan and null values for each column\n",
    "    data_quality_df = data_df.select([count(when(isnan(column), column)).alias(column) for column in data_df.columns])\n",
    "\n",
    "    # Apply results  \n",
    "    if count_records == 0:\n",
    "        print(\"Error: No records available\")\n",
    "        return False\n",
    "    else:\n",
    "        data_quality_report(data_quality_df)\n",
    "        return True\n",
    "    \n",
    "## Execute all validation checks for available dataframes.\n",
    "## In case the validation was succesfully True is returned. Otherwise False.\n",
    "data_quality_tweets_time = data_quality_check(tweets_time_df)\n",
    "data_quality_tweets_user = data_quality_check(tweets_user_df)\n",
    "data_quality_tweets_content = data_quality_check(tweets_content_df)\n",
    "data_quality_marketdata_time = data_quality_check(df_marketdata_time)\n",
    "data_quality_marketdata_trade = data_quality_check(df_marketdata_trade)\n",
    "data_quality_marketdata_symbol = data_quality_check(df_marketdata_symbol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### 4. Persist tweets and marketdata on filesystem\n",
    "If a dataframe passed a quality check the respective data are persisted as parquet file on a filesystem. Hereby, for each dataframe a suitable directory is created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success: Dataframe was loaded into targetsystem\n",
      "Success: Dataframe was loaded into targetsystem\n",
      "Success: Dataframe was loaded into targetsystem\n",
      "Success: Dataframe was loaded into targetsystem\n",
      "Success: Dataframe was loaded into targetsystem\n",
      "Success: Dataframe was loaded into targetsystem\n"
     ]
    }
   ],
   "source": [
    "def load_data(data_df, path, passed_data_quality):\n",
    "    \"\"\"\n",
    "    Description: Load final dataframes from into the targetsystem. Write operations being executed\n",
    "                 as soon as respective data quality check was passed.\n",
    "    Parameters: \n",
    "        data_df: Dataframe being persisted\n",
    "        path: Directory on the filesystem where the data being persisted\n",
    "        passed_data_quality: Boolean - Value depends on the outcome of the data quality check\n",
    "    Returns: \n",
    "        None\n",
    "    \"\"\"    \n",
    "    \n",
    "    # Write dataframe content to file system in case quality checks were succesful (true)\n",
    "    # In case quality checks have not been passed, deny write operation\n",
    "    if passed_data_quality == True:\n",
    "        time_table_parquet = data_df.write.parquet(path=path, \n",
    "                                                   mode=\"overwrite\")\n",
    "    else:\n",
    "        print(\"Error: Data not written into File Storage System. Data Quality check was not passed.\")\n",
    "        \n",
    "    print(\"Success: Dataframe was loaded into targetsystem\")\n",
    "\n",
    "# Define destination path for parquet files\n",
    "path_tweets_time = \"data/tweetsdata/time.parquet\"\n",
    "path_tweets_user = \"data/tweetsdata/user.parquet\"\n",
    "path_tweets_content = \"data/tweetsdata/content.parquet\"\n",
    "path_marketdata_time = \"data/marketdata/time.parquet\"\n",
    "path_marketdata_trade = \"data/marketdata/trade.parquet\"\n",
    "path_marketdata_symbol = \"data/marketdata/symbol.parquet\"\n",
    "\n",
    "## Persist all dataframes in local storage system\n",
    "## In case the validation was succesfully True is returned. Otherwise False.\n",
    "load_data(tweets_time_df, path_tweets_time, data_quality_tweets_time)\n",
    "load_data(tweets_user_df, path_tweets_user, data_quality_tweets_user)\n",
    "load_data(tweets_content_df, path_tweets_content, data_quality_tweets_content)\n",
    "load_data(df_marketdata_time, path_marketdata_time, data_quality_marketdata_time)\n",
    "load_data(df_marketdata_trade, path_marketdata_trade, data_quality_marketdata_trade)\n",
    "load_data(df_marketdata_symbol, path_marketdata_symbol, data_quality_marketdata_symbol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Create Data Dictionary: \n",
    "Print schema Pyspark dataframes instead of creating addtional data dictionary as Pyspark relys on spark dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- open_time: long (nullable = true)\n",
      " |-- open: decimal(20,10) (nullable = true)\n",
      " |-- high: decimal(20,10) (nullable = true)\n",
      " |-- low: decimal(20,10) (nullable = true)\n",
      " |-- close: decimal(20,10) (nullable = true)\n",
      " |-- volume: decimal(20,10) (nullable = true)\n",
      " |-- close_time: long (nullable = true)\n",
      " |-- quote_asset_volume: decimal(20,10) (nullable = true)\n",
      " |-- number_of_trades: integer (nullable = true)\n",
      " |-- taker_buy_base_asset_volume: decimal(20,10) (nullable = true)\n",
      " |-- taker_buy_quote_asset_volume: decimal(20,10) (nullable = true)\n",
      " |-- symbol: string (nullable = false)\n",
      "\n",
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- user: string (nullable = true)\n",
      " |-- favorite_count: long (nullable = true)\n",
      " |-- retweet_count: long (nullable = true)\n",
      " |-- followers_count: long (nullable = true)\n",
      "\n",
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- weekday: integer (nullable = true)\n",
      " |-- weekofyear: integer (nullable = true)\n",
      " |-- datetime: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- user: string (nullable = true)\n",
      " |-- followers_count: long (nullable = true)\n",
      "\n",
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- favorite_count: long (nullable = true)\n",
      " |-- retweet_count: long (nullable = true)\n",
      "\n",
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- weekday: integer (nullable = true)\n",
      " |-- weekofyear: integer (nullable = true)\n",
      " |-- datetime: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- symbol: string (nullable = false)\n",
      "\n",
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- open: decimal(20,10) (nullable = true)\n",
      " |-- close: decimal(20,10) (nullable = true)\n",
      " |-- high: decimal(20,10) (nullable = true)\n",
      " |-- low: decimal(20,10) (nullable = true)\n",
      " |-- quote_asset_volume: decimal(20,10) (nullable = true)\n",
      " |-- number_of_trades: integer (nullable = true)\n",
      " |-- taker_buy_base_asset_volume: decimal(20,10) (nullable = true)\n",
      " |-- taker_buy_quote_asset_volume: decimal(20,10) (nullable = true)\n",
      "\n",
      "+-------------+---------------+---------------+---------------+---------------+----------------+-------------+--------------------+----------------+---------------------------+----------------------------+-------+\n",
      "|    open_time|           open|           high|            low|          close|          volume|   close_time|  quote_asset_volume|number_of_trades|taker_buy_base_asset_volume|taker_buy_quote_asset_volume| symbol|\n",
      "+-------------+---------------+---------------+---------------+---------------+----------------+-------------+--------------------+----------------+---------------------------+----------------------------+-------+\n",
      "|1530403200000|6391.0800000000|6441.0600000000|6251.0000000000|6356.8100000000|27562.9659740000|1530489599999|175096501.4948690700|          183060|           15580.0338480000|         98972791.7126525400|BTCUSDT|\n",
      "|1530489600000|6360.8200000000|6685.0000000000|6271.6200000000|6615.2900000000|36230.4522640000|1530575999999|235524330.6486533500|          242589|           18966.9109070000|        123205676.1927449700|BTCUSDT|\n",
      "|1530576000000|6615.2900000000|6679.3500000000|6463.0100000000|6513.8600000000|38657.7816910000|1530662399999|254550438.1376888300|          232914|           20503.6253990000|        135047837.8167884700|BTCUSDT|\n",
      "|1530662400000|6511.0100000000|6784.9200000000|6441.1100000000|6586.9800000000|23571.8927770000|1530748799999|156277884.3301159600|          146203|           12244.0709080000|         81263604.8551188200|BTCUSDT|\n",
      "|1530748800000|6585.5300000000|6712.0000000000|6453.3300000000|6529.2000000000|35071.9728980000|1530835199999|230809190.3074051300|          216533|           19214.7807020000|        126481460.0087366100|BTCUSDT|\n",
      "+-------------+---------------+---------------+---------------+---------------+----------------+-------------+--------------------+----------------+---------------------------+----------------------------+-------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-------------------+--------------------+--------------------+---------------+--------------+-------------+---------------+\n",
      "|                 id|                date|                text|           user|favorite_count|retweet_count|followers_count|\n",
      "+-------------------+--------------------+--------------------+---------------+--------------+-------------+---------------+\n",
      "|1187881520415027200|Fri Oct 25 23:59:...|RT @digitalassetb...|      XPplumber|             0|            0|             10|\n",
      "|1187881509316976642|Fri Oct 25 23:59:...|RT @alphatrends: ...|     nickchen66|             0|            0|           1208|\n",
      "|1187881501553287168|Fri Oct 25 23:59:...|RT @cfamediadotng...| cybersec_feeds|             0|            0|           4326|\n",
      "|1187881489821659138|Fri Oct 25 23:59:...|#Bitcoin (#Btc) P...| StartBitcoinOK|             0|            0|            621|\n",
      "|1187881473832964097|Fri Oct 25 23:59:...|$BTC | #BTC - bit...|CryptoPressNews|             1|            1|          60525|\n",
      "+-------------------+--------------------+--------------------+---------------+--------------+-------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-------------------+-----+----+-------+----------+-------------------+\n",
      "|                 id|month|year|weekday|weekofyear|           datetime|\n",
      "+-------------------+-----+----+-------+----------+-------------------+\n",
      "|1187881520415027200|   10|2019|      6|        43|2019-10-25 23:59:58|\n",
      "|1187881509316976642|   10|2019|      6|        43|2019-10-25 23:59:56|\n",
      "|1187881501553287168|   10|2019|      6|        43|2019-10-25 23:59:54|\n",
      "|1187881489821659138|   10|2019|      6|        43|2019-10-25 23:59:51|\n",
      "|1187881473832964097|   10|2019|      6|        43|2019-10-25 23:59:47|\n",
      "+-------------------+-----+----+-------+----------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-------------------+---------------+---------------+\n",
      "|                 id|           user|followers_count|\n",
      "+-------------------+---------------+---------------+\n",
      "|1187881520415027200|      XPplumber|             10|\n",
      "|1187881509316976642|     nickchen66|           1208|\n",
      "|1187881501553287168| cybersec_feeds|           4326|\n",
      "|1187881489821659138| StartBitcoinOK|            621|\n",
      "|1187881473832964097|CryptoPressNews|          60525|\n",
      "+-------------------+---------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-------------------+--------------------+--------------+-------------+\n",
      "|                 id|                text|favorite_count|retweet_count|\n",
      "+-------------------+--------------------+--------------+-------------+\n",
      "|1187881520415027200|RT @digitalassetb...|             0|            0|\n",
      "|1187881509316976642|RT @alphatrends: ...|             0|            0|\n",
      "|1187881501553287168|RT @cfamediadotng...|             0|            0|\n",
      "|1187881489821659138|#Bitcoin (#Btc) P...|             0|            0|\n",
      "|1187881473832964097|$BTC | #BTC - bit...|             1|            1|\n",
      "+-------------------+--------------------+--------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-------------+-----+----+-------+----------+-------------------+\n",
      "|           id|month|year|weekday|weekofyear|           datetime|\n",
      "+-------------+-----+----+-------+----------+-------------------+\n",
      "|1530489599999|    7|2018|      1|        26|2018-07-01 23:59:59|\n",
      "|1530575999999|    7|2018|      2|        27|2018-07-02 23:59:59|\n",
      "|1530662399999|    7|2018|      3|        27|2018-07-03 23:59:59|\n",
      "|1530748799999|    7|2018|      4|        27|2018-07-04 23:59:59|\n",
      "|1530835199999|    7|2018|      5|        27|2018-07-05 23:59:59|\n",
      "+-------------+-----+----+-------+----------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-------------+-------+\n",
      "|           id| symbol|\n",
      "+-------------+-------+\n",
      "|1530489599999|BTCUSDT|\n",
      "|1530575999999|BTCUSDT|\n",
      "|1530662399999|BTCUSDT|\n",
      "|1530748799999|BTCUSDT|\n",
      "|1530835199999|BTCUSDT|\n",
      "+-------------+-------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-------------+---------------+---------------+---------------+---------------+--------------------+----------------+---------------------------+----------------------------+\n",
      "|           id|           open|          close|           high|            low|  quote_asset_volume|number_of_trades|taker_buy_base_asset_volume|taker_buy_quote_asset_volume|\n",
      "+-------------+---------------+---------------+---------------+---------------+--------------------+----------------+---------------------------+----------------------------+\n",
      "|1530489599999|6391.0800000000|6356.8100000000|6441.0600000000|6251.0000000000|175096501.4948690700|          183060|           15580.0338480000|         98972791.7126525400|\n",
      "|1530575999999|6360.8200000000|6615.2900000000|6685.0000000000|6271.6200000000|235524330.6486533500|          242589|           18966.9109070000|        123205676.1927449700|\n",
      "|1530662399999|6615.2900000000|6513.8600000000|6679.3500000000|6463.0100000000|254550438.1376888300|          232914|           20503.6253990000|        135047837.8167884700|\n",
      "|1530748799999|6511.0100000000|6586.9800000000|6784.9200000000|6441.1100000000|156277884.3301159600|          146203|           12244.0709080000|         81263604.8551188200|\n",
      "|1530835199999|6585.5300000000|6529.2000000000|6712.0000000000|6453.3300000000|230809190.3074051300|          216533|           19214.7807020000|        126481460.0087366100|\n",
      "+-------------+---------------+---------------+---------------+---------------+--------------------+----------------+---------------------------+----------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Print schemas instead of creating a data dictionary as pyspark works with dataframes.\n",
    "# Dataframes received from the APIs\n",
    "df_marketdata.printSchema()\n",
    "tweets_df.printSchema()\n",
    "\n",
    "# Final dataframes being written into filesystem\n",
    "tweets_time_df.printSchema()\n",
    "tweets_user_df.printSchema() \n",
    "tweets_content_df.printSchema() \n",
    "df_marketdata_time.printSchema()\n",
    "df_marketdata_symbol.printSchema()\n",
    "df_marketdata_trade.printSchema()\n",
    "\n",
    "## Extract from source and final tables \n",
    "df_marketdata.show(5)\n",
    "tweets_df.show(5)\n",
    "\n",
    "# Final dataframes being written into filesystem\n",
    "tweets_time_df.show(5)\n",
    "tweets_user_df.show(5) \n",
    "tweets_content_df.show(5)\n",
    "df_marketdata_time.show(5)\n",
    "df_marketdata_symbol.show(5)\n",
    "df_marketdata_trade.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import tarfile\n",
    "\n",
    "def recursive_files(dir_name='.', ignore=None):\n",
    "    for dir_name,subdirs,files in os.walk(dir_name):\n",
    "        if ignore and os.path.basename(dir_name) in ignore: \n",
    "            continue\n",
    "\n",
    "        for file_name in files:\n",
    "            if ignore and file_name in ignore:\n",
    "                continue\n",
    "\n",
    "            yield os.path.join(dir_name, file_name)\n",
    "\n",
    "def make_tar_file(dir_name='.', tar_file_name='tarfile.tar', ignore=None):\n",
    "    tar = tarfile.open(tar_file_name, 'w', dereference=True)\n",
    "\n",
    "    for file_name in recursive_files(dir_name, ignore):\n",
    "        tar.add(file_name)\n",
    "\n",
    "    tar.close()\n",
    "\n",
    "\n",
    "dir_name = '.'\n",
    "tar_file_name = 'archive.tar'\n",
    "ignore = {'.ipynb_checkpoints', '__pycache__', tar_file_name}\n",
    "make_tar_file(dir_name, tar_file_name, ignore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
